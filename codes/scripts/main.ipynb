{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    " # commented for local testing<br>\n",
    "# For Legal Pythia<br>\n",
    "# @author: SURYA L RAMESH<br>\n",
    "# First Created on Thu May 27 17:28:25 2021<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # SECTION 1. IMPORT ALL REQUIREMENTS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # commented for local testing<br>\n",
    "# from __future__ import unicode_literals # moved to the top<br>\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from annotated_text import annotated_text\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader #SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AlbertTokenizer\n",
    "from transformers import AlbertForSequenceClassification, AdamW\n",
    "nlp = spacy.load('en_core_web_sm') # large needed for word vectors\n",
    "path = os.path.abspath(os.getcwd())<br>\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"\\n >>> device used: \",device)\n",
    "print (\"\\n\")\n",
    "device_type = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# SECTION 2. THE MODEL\n",
    "\n",
    "'' # commented for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to() received an invalid combination of arguments - got (type), but expected one of:\n * (torch.device device, torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (Tensor tensor, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlbertForSequenceClassification\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AlbertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbert-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\LegalPythia-V2-legal-pythia-v2.1\\lib\\site-packages\\torch\\nn\\modules\\module.py:908\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves and/or casts the parameters and buffers.\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \n\u001b[0;32m    826\u001b[0m \u001b[38;5;124;03m    This can be called as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    905\u001b[0m \n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dtype\u001b[38;5;241m.\u001b[39mis_floating_point \u001b[38;5;129;01mor\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex):\n",
      "\u001b[1;31mTypeError\u001b[0m: to() received an invalid combination of arguments - got (type), but expected one of:\n * (torch.device device, torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (Tensor tensor, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification\n",
    "\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " contains all of the hyperparemeter information for training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m(optimizer_grouped_parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, correct_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m epoch_to_resume \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      2\u001b[0m path_to_model_saved \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_epoch\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat( epoch_to_resume)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_to_model_saved):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m >>> loading checkpoint \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path_to_model_saved))\n\u001b[0;32m      5\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_model_saved,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_to_resume = 4\n",
    "path_to_model_saved = 'model_epoch{}.pt'.format( epoch_to_resume)\n",
    "if os.path.isfile(path_to_model_saved):\n",
    "    print(\"\\n >>> loading checkpoint '{}'\".format(path_to_model_saved))\n",
    "    checkpoint = torch.load(path_to_model_saved,map_location=torch.device('cpu'))\n",
    "    savd_epoch = checkpoint['epoch']\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"\\n >>> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(path_to_model_saved, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"\\n >>> no checkpoint found at '{}'\".format(path_to_model_saved))\n",
    "    \n",
    "    \n",
    "    \n",
    "# ''''''''''''''''''''''' SECTION 3. THE FUNCTIONS ''''''''''''''''''''''''''''' # commented for local testing\n",
    "       \n",
    "def calculate_similarity_percentage(file1, file2):\n",
    "    \n",
    "    # spaCy has support for word vectors whereas NLTK does not\n",
    "    \n",
    "    text1 = nlp(file1)\n",
    "    text2 = nlp(file2)   \n",
    "             \n",
    "    sim = text1.similarity(text2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3130198992.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [17]\u001b[1;36m\u001b[0m\n\u001b[1;33m    class SNLIDataAlbertPredictor(Dataset):\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class SNLIDataAlbertPredictor(Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __init__(self,input_df):\n",
    "    self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    self.input_df = input_df\n",
    "    self.base_path = '/content/'\n",
    "    self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
    "    self.input_data = None\n",
    "  \n",
    "    self.init_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def init_data(self):\n",
    "    \n",
    "    self.input_data = self.load_data(self.input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def load_data(self, df):\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    premise_list = df['premise'].to_list()\n",
    "    hypothesis_list = df['hypothesis'].to_list()\n",
    "  \n",
    "    for (premise, hypothesis) in zip(premise_list, hypothesis_list):\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "    \n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "   \n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids)\n",
    "    #print(len(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    input_loader = DataLoader(\n",
    "      self.input_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "    return input_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for checking similarity and contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "def check_similarity_contradiction(sentence1, sentence2):\n",
    "           \n",
    "       data_input = {'premise':[sentence1], 'hypothesis':[sentence2]}\n",
    "       df_input = pd.DataFrame(data_input, columns = ['premise','hypothesis'])\n",
    "       \n",
    "       input_dataset = SNLIDataAlbertPredictor(df_input)\n",
    "       input_loader = input_dataset.get_data_loaders(batch_size=1)\n",
    "       \n",
    "       (pair_token_ids, mask_ids, seg_ids) = next(iter(input_loader))\n",
    "       pair_token_ids = pair_token_ids.to(device)\n",
    "       mask_ids = mask_ids.to(device)\n",
    "       seg_ids = seg_ids.to(device)\n",
    "       result = model(pair_token_ids, \n",
    "                                     token_type_ids=seg_ids, \n",
    "                                     attention_mask=mask_ids)\n",
    "       prediction = result.logits #Predition in tensor Form\n",
    "       softmax =torch.log_softmax(prediction, dim=1)\n",
    "       pred =softmax.argmax(dim=1)\n",
    "       \n",
    "       target_map = {0: 'entailment',1:'contradiction',2:'neutral'}\n",
    "       \n",
    "       if device_type == \"cpu\":\n",
    "          outcome = target_map[pred.data.cpu().numpy()[0]]\n",
    "       else:\n",
    "           outcome = target_map[pred[0]]  # modified to get value from tensor\n",
    "       return  outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styler(col):\n",
    "    # apply style to prediction column only \n",
    "    if col.name != 'prediction':\n",
    "        return [''] * len(col)\n",
    "    bg_map = []\n",
    "    \n",
    "    for x in col:\n",
    "        \n",
    "        if x[0] == 'contradiction' :\n",
    "                bg_map.append ('background-color:LightCoral')\n",
    "        elif x[0] == 'entailment' :\n",
    "                bg_map.append( 'background-color:LightGreen')\n",
    "        else:\n",
    "                bg_map.append('')\n",
    "    \n",
    "        \n",
    "    #print (bg_map)\n",
    "    return bg_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file):\n",
    "    pdf = pdfplumber.open(file)\n",
    "    page = pdf.pages[0]\n",
    "    text = page.extract_text()\n",
    "    pdf.close()\n",
    "    return text.encode('utf8')\n",
    "                \n",
    "def doc_to_text(file):\n",
    "    text = docx2txt.process(file)\n",
    "    text = text.replace('\\n\\n',' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_ner(text):\n",
    "    tokens = []\n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        if (token.ent_type_ == \"PERSON\"):\n",
    "            tokens.append((token.text, \"PERSON\", \"#faa\"))\n",
    "        elif (token.ent_type_ == \"LOC\"):\n",
    "            tokens.append((token.text, \"LOC\", \"#fda\"))\n",
    "        elif (token.ent_type_ == \"GPE\"):\n",
    "            tokens.append((token.text, \"GPE\", \"#be2\"))\n",
    "        elif (token.ent_type_ == \"ORG\"):\n",
    "            tokens.append((token.text, \"ORG\", \"#0cf\"))\n",
    "        elif (token.ent_type_ == \"DATE\"):\n",
    "            tokens.append((token.text, \"DATE\", \"#fd1\"))\n",
    "        elif (token.ent_type_ == \"MONEY\"):\n",
    "            tokens.append((token.text, \"MONEY\", \"#f1d\"))    \n",
    "        else:\n",
    "            tokens.append(\" \" + token.text + \" \")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def visualise_ner(file1, file2):<br>\n",
    "    text1 = nlp(file1)<br>\n",
    "    text2 = nlp(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    annotated_text(text1)<br>\n",
    "    annotated_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " SECTION 4. THE MAIN APP CODE \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # commented for local testing<br>\n",
    "       <br>\n",
    "def main():<br>\n",
    "    header = st.container() # updated st.beta_container() to st.container()<br>\n",
    "    steps = st.container() # updated st.beta_container() to st.container()<br>\n",
    "    userinputfiles = st.container() # updated st.beta_container() to st.container()<br>\n",
    "    userchoice = st.container() # updated st.beta_container() to st.container()<br>\n",
    " <br>\n",
    "    # -- Default selector list<br>\n",
    "    selector_list = ['Similarity %','Similarity and Contradition detection', 'Visualise Entities']<br>\n",
    "    with header:<br>\n",
    "        st.image('/Users/gayanin/RA-Work/Legal Pythia/LegalPythia-V2/codes/res/header.jpeg')<br>\n",
    "        st.title(' Welcome to Legal Pythia Demo!')<br>\n",
    "        st.text(' Here you get to upload two text files and check for similarity or contradiction')<br>\n",
    "    # with steps:<br>\n",
    "    #     st.subheader('The Three Step Process:')<br>\n",
    "        <br>\n",
    "    #     st.markdown ('* ** Step 1:** Load document 1' )    <br>\n",
    "    #     st.markdown ('* ** Step 2:** Load document 2' )<br>\n",
    "    #     st.markdown ('* ** Step 3:** Choose Similarity %  or Similarity and Contradiction Detection or Visualisation')<br>\n",
    "        <br>\n",
    "    <br>\n",
    "    # with userchoice:<br>\n",
    "        # userchoice = st.radio(\"Choose your comparison method\",('Similarity % ','Similarity and Contradition detection', 'Visualise Entities'))<br>\n",
    "        # if userchoice == 'Similarity % ':<br>\n",
    "        #         st.write('You have selected Similarity.')<br>\n",
    "        # if userchoice == 'Similarity and Contradition detection':<br>\n",
    "        #     st.write('You have selected Similarity and Contradition detection.')<br>\n",
    "        # if userchoice == 'Visualise Entities':<br>\n",
    "        #         st.write('\\n You have selected Visualisation.')<br>\n",
    "        <br>\n",
    "        # selector = st.sidebar.selectbox('Selector', selector_list)<br>\n",
    "    with userinputfiles and userchoice:<br>\n",
    "       <br>\n",
    "        # sel_col, disp_col = st.sidebar.columns(2) # updated st.beta_columns() to st.columns()<br>\n",
    "        file1 = st.sidebar.file_uploader(\"Upload first document\", type = ['txt','pdf','docx'])<br>\n",
    "        file2 = st.sidebar.file_uploader(\"Upload second document\", type = ['txt','pdf','docx'])<br>\n",
    "        print(\"Document1...................................\",file1)<br>\n",
    "        print(\"Document2...............................\",file2)<br>\n",
    "        userchoice = st.sidebar.selectbox('Setect the feature function', selector_list)<br>\n",
    "        # st.write('You selected:', userchoice)<br>\n",
    "        # if file1 is not None:<br>\n",
    "        #     premise_text = file1.read()       <br>\n",
    "        #     premises = nltk.sent_tokenize(premise_text.decode('utf8')) # bytes to string<br>\n",
    "        # if file2 is not None:<br>\n",
    "        #     hypothesis_text = file2.read()           <br>\n",
    "        #     hypotheses = nltk.sent_tokenize(hypothesis_text.decode('utf8')) # bytes to string<br>\n",
    "        if file1 is not None:<br>\n",
    "            if 'pdf' in file1.name:<br>\n",
    "               premise_text = pdf_to_text(file1) <br>\n",
    "            elif 'doc' in file1.name:<br>\n",
    "               premise_text = doc_to_text(file1)  <br>\n",
    "            else:<br>\n",
    "                premise_text = file1.read()       <br>\n",
    "            premises = nltk.sent_tokenize(premise_text.decode('utf8')) # bytes to string<br>\n",
    "            <br>\n",
    "        if file2 is not None:<br>\n",
    "            if 'pdf' in file2.name:<br>\n",
    "               hypothesis_text = pdf_to_text(file2) <br>\n",
    "            elif '.doc' in file2.name:<br>\n",
    "               hypothesis_text = doc_to_text(file2) <br>\n",
    "            else:<br>\n",
    "                hypothesis_text = file2.read()             <br>\n",
    "            hypotheses = nltk.sent_tokenize(hypothesis_text.decode('utf8')) # bytes to string<br>\n",
    "        if(file1 is not None) and  (file2 is not None) and userchoice == 'Similarity %':                       <br>\n",
    "           sim = calculate_similarity_percentage(premise_text.decode('utf8'),hypothesis_text.decode('utf8'))<br>\n",
    "        #    st.write(sim)<br>\n",
    "           sim_percent = \"{:.0%}\".format(sim)<br>\n",
    "           st.write (\"\\n The similarity of two documents is \", sim_percent)<br>\n",
    "           sim_p = 1 - sim<br>\n",
    "           #draw a pie chart<br>\n",
    "           plot_labels = 'Similarity %', 'Contradiction %'<br>\n",
    "           plot_sizes = [sim, sim_p]<br>\n",
    "        #    explode = (0.1, 0) <br>\n",
    "           colours = ['#81ef7d','#ea696d']<br>\n",
    "    <br>\n",
    "           fig1, ax1 = plt.subplots()<br>\n",
    "           ax1.pie(plot_sizes, colors=colours, labels=plot_labels, autopct='%1.1f%%',<br>\n",
    "           shadow=True, startangle=90)<br>\n",
    "           ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.<br>\n",
    "           st.pyplot(fig1, transparent=True)<br>\n",
    "           <br>\n",
    "        if(file1 is not None) and  (file2 is not None) and userchoice == 'Similarity and Contradition detection':         <br>\n",
    "            st.text('File upload successful!.')<br>\n",
    "            st.text('Checking for Similarity and Contradictions...')<br>\n",
    "            my_bar = st.progress(0)<br>\n",
    "            for percent_complete in range(100):<br>\n",
    "                time.sleep(0.1)<br>\n",
    "            my_bar.progress(percent_complete + 1)<br>\n",
    "            df_output = pd.DataFrame(columns = ['premise', 'hypothesis', 'prediction'])<br>\n",
    "                             <br>\n",
    "            row_count = 0<br>\n",
    "            <br>\n",
    "            # Add a placeholder for progress bar<br>\n",
    "            checking_text = st.empty()<br>\n",
    "            bar = st.progress(0)<br>\n",
    "                       <br>\n",
    "                      <br>\n",
    "            totalCount = len(premises)  * len(hypotheses)<br>\n",
    "            for premise in premises:<br>\n",
    "                for hypothesis in hypotheses:<br>\n",
    "                    outcome = check_similarity_contradiction(premise, hypothesis) <br>\n",
    "                    row = {'premise':premise, 'hypothesis':hypothesis,'prediction':[outcome]}<br>\n",
    "                    row_count = row_count + 1<br>\n",
    "                    df_output =  df_output.append( row , ignore_index=True)<br>\n",
    "                    print(\"Row = \", row)<br>\n",
    "                    <br>\n",
    "                     # Update the progress bar <br>\n",
    "                    checking_text.text(f'Processing Similarity and Contradiction Detection...  {row_count} of {totalCount}')<br>\n",
    "                    bar.progress((row_count/totalCount))<br>\n",
    "                    time.sleep(0.1)<br>\n",
    "               <br>\n",
    "            streamlit_df = pd.DataFrame(df_output)<br>\n",
    "            df_output.to_csv('predictions.csv')    <br>\n",
    "            st.dataframe(streamlit_df.style.apply(styler))<br>\n",
    "            <br>\n",
    "            @st.cache<br>\n",
    "            def convert_df_to_csv(df):<br>\n",
    "                # IMPORTANT: Cache the conversion to prevent computation on every rerun<br>\n",
    "                return df.to_csv().encode('utf-8')<br>\n",
    "            st.download_button(<br>\n",
    "                label=\"Download data as CSV\",<br>\n",
    "                data=convert_df_to_csv(streamlit_df),<br>\n",
    "                file_name='document_comparison.csv',<br>\n",
    "                mime='text/csv',<br>\n",
    "            )<br>\n",
    "        if(file1 is not None) and  (file2 is not None) and userchoice == 'Visualise Entities':         <br>\n",
    "            st.write('Document 1: \\n')<br>\n",
    "            premise_tokens = visualise_ner(premise_text.decode('utf8'))<br>\n",
    "            annotated_text(*premise_tokens)<br>\n",
    "            st.write('\\n')<br>\n",
    "            st.write('Document 2: \\n')<br>\n",
    "            hypothesis_tokens = visualise_ner(hypothesis_text.decode('utf8'))<br>\n",
    "            annotated_text(*hypothesis_tokens)<br>\n",
    "            st.write('\\n')<br>\n",
    "    <br>\n",
    "    if \"load_state\" not in st.session_state:<br>\n",
    "       st.session_state.load_state = False<br>\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")<br>\n",
    "if __name__ == \"__main__\":<br>\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
